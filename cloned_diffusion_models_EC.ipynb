{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hands on Diffusion model\n",
    "\n",
    "[Diffusion models](https://arxiv.org/pdf/2006.11239.pdf) are a family of models that have shown amazing capability of generating photorealistic images with/ without text prompt. They have two flows as shown in the figure below - \n",
    "1. Deterministic forward flow (from image to noise) and \n",
    "2. Generative reverse flow (recreating image from noise).\n",
    "\n",
    "Diffusion models get their name from the forward flow where they follow a markov chain of diffusion steps, each of which adds a small amount of random noise to the data. Then they learn the model to reverse the diffusion process and construct desired data samples from noise. \n",
    "\n",
    "<figure>\n",
    "<p style=\"text-align:center;\"  align = \"center\"><img src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2022/04/Fixed_Forward_Diffusion_Process.png\" alt=\"Trulli\" style=\"width:100%\"  align = \"center\"></p>\n",
    "<figcaption align = \"center\">Forward and reverse process <a href=\"https://developer.nvidia.com/blog/improving-diffusion-models-as-an-alternative-to-gans-part-1/\">Ref: Nvidia blog</a> </figcaption>\n",
    "</figure>\n",
    " \n",
    "\n",
    "\n",
    "Since they map noise to data, these models can be said to be capable of learning the distributions that generate data of any particular domain.\n",
    "\n",
    "This notebook showcases a minimal example of the forward diffusion process and its reverse mapping using a dense network. It is meant to give the reader side by side code snippets to match the equations in the paper and visual examples of the complete process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title\n",
    "! pip install celluloid\n",
    "import math\n",
    "import tensorflow as tf\n",
    "\n",
    "from random import randrange\n",
    "\n",
    "import array as arr\n",
    "\n",
    "import numpy as np\n",
    "from numpy import asarray, ndarray\n",
    "from IPython.display import HTML, Image, display\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image, ImageDraw\n",
    "\n",
    "from celluloid import Camera\n",
    "import functools\n",
    "import sklearn.datasets\n",
    "\n",
    "# For plotting\n",
    "from IPython.display import HTML\n",
    "from base64 import b64encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title\n",
    "# Utility function for displaying video inline in colab\n",
    "\n",
    "def show_video(vname):\n",
    "  mp4 = open(vname,'rb').read()\n",
    "  data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
    "  return HTML(\"\"\"\n",
    "  <video width=400 controls>\n",
    "        <source src=\"%s\" type=\"video/mp4\">\n",
    "  </video>\n",
    "  \"\"\" % data_url)\n",
    "\n",
    "def save_animation(vname, interval=30):\n",
    "  anim = camera.animate(blit=True, interval=interval)\n",
    "  anim.save(vname)\n",
    "\n",
    "# Utility function for random noise\n",
    "def noise_like(shape):\n",
    "  return tf.random.normal(shape=shape, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function for random noise\n",
    "def noise_like(shape):\n",
    "  return tf.random.normal(shape=shape, dtype=tf.float32)\n",
    "\n",
    "all_data = []\n",
    "one_d_image_data = []\n",
    "\n",
    "#imagining a for i in [allimages]\n",
    "\n",
    "  #image.open \n",
    "  #image array it\n",
    "  #normalize\n",
    "    #?????\n",
    "  #turn normalized into one dimensional list \n",
    "  #append to alldata\n",
    "  #DONT empty one dimension list\n",
    "#DONT set alldata to zero \n",
    "#it messed me up later because of how datalist and one dimension list get handled or turned into tf objects. dont mess with this stuff yet\n",
    "\n",
    "myImage = Image.open(\"./pixel_one.png\").convert('L')\n",
    "# plt.pcolor(myImage)\n",
    "#5 width, 5 height\n",
    "#original image is transparent but gets black background once greyscaled\n",
    "\n",
    "greyscale_array = asarray(myImage)\n",
    "# print(\"greyscale array \")\n",
    "# print(greyscale_array)\n",
    "# plt.pcolor(greyscale_array)\n",
    "\n",
    "#it's a grey and black image. the background is black; the one is\n",
    "\n",
    "stat_normalized_array = (greyscale_array - greyscale_array.min()) * ((0.5+ 0.5)/(greyscale_array.max()- greyscale_array.min()))# - 0.5 \n",
    "#Normalized Image = (Original image - min of image) * ((newMax-newMin) / (ImageMax - ImageMin)) + newMin\n",
    "# 0.5 + 0.5 because 0.5 - (-0.5)\n",
    "print(\"normalized_array \")\n",
    "print(stat_normalized_array)\n",
    "plt_obj=plt.pcolor(stat_normalized_array)\n",
    "plt.colorbar(plt_obj)\n",
    "\n",
    "# EC: No reason to convert an array to a list. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beta schedule\n",
    "\n",
    "Now that we have the original (non noisy) data, let's start now with the actual diffusion implementation. The first thing is to add noise to the input images following a fixed variance schedule (also known as beta schedule). The original paper uses a linear schedule. And 1000 timesteps to move forward and back. We use smaller number of timesteps (250) as the data is simpler in our case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_diffusion_timesteps=10\n",
    "beta_start=0.0001\n",
    "beta_end=0.02\n",
    "schedule_type='quadratic'\n",
    "\n",
    "def get_beta_schedule(schedule_type, beta_start, beta_end, num_diffusion_timesteps):\n",
    "  if schedule_type == 'quadratic':\n",
    "    betas = np.linspace(beta_start ** 0.5, beta_end ** 0.5, num_diffusion_timesteps, dtype=np.float32) ** 2\n",
    "  elif schedule_type == 'linear':\n",
    "    betas = np.linspace(beta_start, beta_end, num_diffusion_timesteps, dtype=np.float32)\n",
    "  print(betas)\n",
    "  print(type(betas))\n",
    "  print(betas.shape)\n",
    "  return betas\n",
    "\n",
    "betas_linear = get_beta_schedule('linear', beta_start, beta_end, num_diffusion_timesteps)\n",
    "betas_quad = get_beta_schedule('quadratic', beta_start, beta_end, num_diffusion_timesteps)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize beta schedules\n",
    "\n",
    "The below plot shows that the variance of noise is low at the start and increases as we move forward in time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title\n",
    "\n",
    "plt.plot(betas_linear, label = 'linear')\n",
    "plt.plot(betas_quad, label='quad')\n",
    "plt.title('Beta schedule')\n",
    "plt.ylabel('Beta value')\n",
    "plt.xlabel('Timestep')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beta derivatives\n",
    "\n",
    "Next, let's compute all the derivatives from beta that are used repeatedly in the forward and reverse process of diffusion. Since the variance schedule ($\\beta_t$) is fixed, the derivatives of $\\beta_t$ are also fixed. We precompute these to save time/ compute.\n",
    "\n",
    "We'll see the use cases of these variables in the respective sections below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BetaDerivatives():\n",
    "  def __init__(self, betas, dtype=tf.float32):\n",
    "    \"\"\"Take in betas and pre-compute the dependent values to use in forward/ backward pass.\n",
    "    \n",
    "    Values are precomputed for all timesteps so that they can be used as and\n",
    "    when required.\n",
    "    \"\"\"\n",
    "    self.np_betas = betas\n",
    "    timesteps, = betas.shape\n",
    "    print(betas.shape)\n",
    "    self.num_timesteps = int(timesteps)\n",
    "\n",
    "    self.betas = tf.constant(betas, dtype=dtype)\n",
    "    self.alphas = tf.subtract(1., betas)\n",
    "    self.alphas_cumprod = tf.math.cumprod(self.alphas, axis=0)\n",
    "    self.alphas_cumprod_prev = tf.concat([tf.constant([1.0]), self.alphas_cumprod[:-1]], axis=0)\n",
    "\n",
    "    # calculations required for diffusion q(x_t | x_{t-1}) and others\n",
    "    self.sqrt_alphas_cumprod = tf.math.sqrt(self.alphas_cumprod)\n",
    "    self.sqrt_one_minus_alphas_cumprod = tf.math.sqrt(1. - self.alphas_cumprod)\n",
    "    self.log_one_minus_alphas_cumprod = tf.math.log(1. - self.alphas_cumprod)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  def _gather(self, a, t):\n",
    "    \"\"\"\n",
    "    Utility function to extract some coefficients at specified timesteps,\n",
    "    then reshape to [batch_size, 1] for broadcasting.\n",
    "    \"\"\"\n",
    "    return tf.reshape(tf.gather(a, t), [-1, 1])\n",
    "    # return tf.gather(a, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdb = BetaDerivatives(betas_quad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize beta derivatives over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title\n",
    "# Visualizing betas and other variables\n",
    "plt.figure(figsize=(16, 9))\n",
    "\n",
    "plt.subplot(2,4,1)\n",
    "plt.plot(gdb.betas)\n",
    "plt.title('Betas')\n",
    "plt.subplot(2,4,2)\n",
    "plt.plot(gdb.alphas)\n",
    "plt.title('Alphas')\n",
    "\n",
    "plt.subplot(2,4,3)\n",
    "plt.plot(gdb.alphas_cumprod, label='alphas_cumprod')\n",
    "plt.plot(gdb.sqrt_alphas_cumprod, label='sqrt_alphas_cumprod')\n",
    "plt.legend();\n",
    "plt.subplot(2,4,4)\n",
    "plt.plot(1-gdb.alphas_cumprod, label='one_minus_alphas_cumprod')\n",
    "plt.plot(gdb.sqrt_one_minus_alphas_cumprod, label='sqrt_one_minus_alphas_cumprod')\n",
    "plt.plot(gdb.log_one_minus_alphas_cumprod, label='log_one_minus_alphas_cumprod')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward pass of diffusion model\n",
    "\n",
    "In the forward pass, the diffused input at timestep t can be computed directly using the closed form equation (For derivation of how we arrive at this, refer to the paper).\n",
    "\n",
    "$q(x_t| x_0) = N(\\sqrt{\\bar{\\alpha_t}}x_o, 1-\\bar{\\alpha_t}I)$\n",
    "\n",
    "This is done in the q_sample function below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make this work with one-dimensional data set \n",
    "\n",
    "\n",
    "class DiffusionForward(BetaDerivatives):\n",
    "  \"\"\"\n",
    "  Forward pass of the diffusion model.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, betas):\n",
    "    super().__init__(betas)\n",
    "    \n",
    "\n",
    "  def q_sample(self, x_start, t, noise=None):\n",
    "    \"\"\"\n",
    "    Forward pass - sample of diffused data at time t.\n",
    "    \"\"\"\n",
    "    if noise is None:\n",
    "      noise = tf.random.normal(shape=x_start.shape)\n",
    "    p1 = self._gather(self.sqrt_alphas_cumprod, t) * x_start\n",
    "    # print(self._gather(self.sqrt_alphas_cumprod, t), x_start, p1)\n",
    "    p2 = self._gather(self.sqrt_one_minus_alphas_cumprod, t) * noise \n",
    "    return (p1 + p2)\n",
    "\n",
    "diff_forward = DiffusionForward(betas_quad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the forward diffusion of the entire data over time\n",
    "\n",
    "We start with original data distribution and move it through the forward diffusion process 10 steps at a time. We can see that the original data distribution information is lost till it resembles gaussian after num_diffusion_steps. \n",
    "\n",
    "Also, the slow perturbations at the start and large ones towards the end as per the beta schedule are evident from the video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 1000\n",
    "\n",
    "all_data = []\n",
    "# Data for all clusters\n",
    "for idx in range(num_samples):\n",
    "  rndm_image = tf.random.uniform(shape=(1,25), minval=0.0, maxval=1.0, dtype=tf.float32) \n",
    "  # print(rndm_image)\n",
    "  all_data.append(rndm_image)\n",
    "\n",
    "# X,_ = sklearn.datasets.make_moons(8000)\n",
    "train_data_tf = tf.concat(all_data, axis=0)\n",
    "print(f'{train_data_tf.shape[0]} samples of {train_data_tf.shape[1]} elements in training data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose a random sample from train_data_rf and visualize it.\n",
    "rndm_idx = tf.random.uniform(shape=[1], minval=0, maxval=999, dtype=tf.int32).numpy()\n",
    "print(rndm_idx)\n",
    "print(train_data_tf[rndm_idx[0]])\n",
    "plt_obj = plt.pcolor(train_data_tf.numpy()[rndm_idx[0],:].reshape((5,5)))\n",
    "# plt_obj = plt.pcolor(train_data_tf.numpy()[0,:].reshape((5,5)))\n",
    "plt.colorbar(plt_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title\n",
    "\n",
    "camera = Camera(plt.figure())\n",
    "\n",
    "x0 = train_data_tf[:]\n",
    "print(x0.shape[0])\n",
    "for timestep in range(0, num_diffusion_timesteps, 1): \n",
    "  tstep = tf.repeat(tf.constant(timestep), (x0.shape[0]))\n",
    "  shifted = diff_forward.q_sample(x0, tstep)\n",
    "  print(timestep, shifted)\n",
    "  plt_obj = plt.pcolor(shifted[0].numpy().reshape((5,5)))\n",
    "  plt.title(f'Time step: {timestep}')\n",
    "  camera.snap()\n",
    "\n",
    "save_animation('image0.mp4', 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_video('image0.mp4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model building\n",
    "\n",
    "With the data taken care of, let's build a model that can fit the data. We use a DNN with few layers since we're just using data with 2 features that we wish to reconstruct. Would be replaced with unet with similar loss function for the case of image data.\n",
    "\n",
    "The model takes in 2 inputs:\n",
    "* Timestep embedding of $t$\n",
    "* $x_t$\n",
    "\n",
    "And predicts \n",
    "* The noise $n$ that lead from $x_0$ to $x_t$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Timestep embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create a 128 dimensional embedding for the timestep input to the model. \n",
    "# Fixed embeddings similar to positional embeddings in transformer are used - \n",
    "# could be replaced by trainable embeddings later\n",
    "\n",
    "#timesteps way up there\n",
    "#embedding_dim, maybe it just gets initialized here. seems that way\n",
    "\n",
    "def get_timestep_embedding(timesteps, embedding_dim: int):\n",
    "  half_dim = embedding_dim // 2\n",
    "  emb = tf.math.log(10000.0) / (half_dim - 1)\n",
    "  \n",
    "  #POSITION in time is a vector = function(t)\n",
    "  #some power of (i), the embedding dimension \n",
    "  #128 dimensions = 0 to 127 dimensions\n",
    "\n",
    "  emb = tf.exp(tf.range(half_dim, dtype=tf.float32) * -emb)\n",
    "  emb = tf.cast(timesteps, dtype=tf.float32)[:, None] * emb[None, :]\n",
    "  emb = tf.concat([tf.sin(emb), tf.cos(emb)], axis=1)\n",
    "  if embedding_dim % 2 == 1:  # zero pad\n",
    "    # emb = tf.concat([emb, tf.zeros([num_embeddings, 1])], axis=1)\n",
    "    emb = tf.pad(emb, [[0, 0], [0, 1]])\n",
    "  return emb\n",
    "\n",
    "# temb = get_timestep_embedding(tf.repeat(tf.constant([1]),25),128)\n",
    "temb = get_timestep_embedding(np.arange(1, 26, 1),128)\n",
    "print(temb.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#take a sample of a REAL image from your entire set of real images.\n",
    "\n",
    "# random_number_timesteps = randrange(0, num_diffusion_timesteps)\n",
    "# print(random_number_timesteps)\n",
    "# generate_random_image(train_data_tf, num_diffusion_timesteps)\n",
    "\n",
    "#sample noise LEVEL t between 1 and T (not necessarily max t)\n",
    "\n",
    "\n",
    "\n",
    "#sample noise from gaussian distribution and corrupt input (first/former sample) by sampled noise/noise at t\n",
    "\n",
    "#neural network predicts future noise by equating your newly produced corrupted image (steps 1-3) to calculable noise (given Beta is known and t is known, regardless of how it was randomly chosen) applied to original image "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actual model that takes in x_t and t and outputs n_{t-1}\n",
    "# Experiments showed that prediction of n_{t-1} worked better compared to\n",
    "# prediction of x_{t-1}\n",
    "\n",
    "print(train_data_tf.shape)\n",
    "\n",
    "def build_model():\n",
    "  input_x = tf.keras.layers.Input(train_data_tf.shape[1]) #.shape[1], shape[0] probably worked because it looked in the tensor for shape and only needed the first parameter\n",
    "  temb = tf.keras.layers.Input(128)\n",
    "\n",
    "  # temb = tf.keras.layers.Reshape((128,))(tf.keras.layers.Embedding(1000, 128)(input_t))\n",
    "  d1 = tf.keras.layers.Dense(128)(input_x)\n",
    "  merge = tf.keras.layers.Concatenate()((temb, d1))\n",
    "  d2 = tf.keras.layers.Dense(128, 'relu')(merge)\n",
    "  d2 = tf.keras.layers.Dense(64, 'relu')(d2)\n",
    "  d2 = tf.keras.layers.Dense(32, 'relu')(d2)\n",
    "  d3 = tf.keras.layers.Dense(25, 'relu')(d2)\n",
    "  model = tf.keras.Model([input_x, temb], d3)\n",
    "  # d3 = tf.keras.layers.Dense(16, 'relu')(d2)\n",
    "  # d4 = tf.keras.layers.Dense(2)(d3)\n",
    "  # model = tf.keras.Model([input_x, temb], d4)\n",
    "  return model\n",
    "\n",
    "model = build_model()\n",
    "print(model.summary())\n",
    "model.compile(loss='mse', optimizer='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data generation for diffusion model\n",
    "\n",
    "Next, let's generate the data for the model to train. We generate $x_t$ given the input $x_0$ using the deterministic forward process equation described above. This $x_t$ and timestep embedding of \n",
    "$t$ are input to the model that is tasked with predicting the noise $n$.\n",
    "\n",
    "$t$ is picked uniformly between [0, num_diffusion_timesteps]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffle_buffer_size = 500\n",
    "batch_size = 32\n",
    "\n",
    "def data_generator_forward(x, gdb):\n",
    "  tstep = tf.random.uniform(shape=(tf.shape(x)[0],), minval=0, maxval=num_diffusion_timesteps, dtype=tf.int32)\n",
    "  print(tf.shape(x)[0])\n",
    "  noise = tf.random.normal(shape = tf.shape(x), dtype=x.dtype)\n",
    "  noisy_out = gdb.q_sample(x, tstep, noise)\n",
    "  return ((noisy_out, get_timestep_embedding(tstep, 128)), noise)\n",
    "\n",
    "# Model takes in noisy output and timestep embedding and predicts noise\n",
    "# print(train_data_tf[0,:])\n",
    "dataset = tf.data.Dataset.from_tensor_slices((train_data_tf)).shuffle(shuffle_buffer_size).batch(batch_size)\n",
    "# for element in dataset:\n",
    "#   print(element)\n",
    "# print( tf.data.Dataset.from_tensor_slices((train_data_tf)).shuffle(shuffle_buffer_size)[0,:] )\n",
    "dataset = dataset.map(functools.partial(data_generator_forward, gdb=diff_forward))\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's test the data generator\n",
    "# print(next(iter(dataset)).shape)\n",
    "(xx,tt),yy = next(iter(dataset))\n",
    "print(xx.shape, tt.shape, yy.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 200\n",
    "history = model.fit(dataset, epochs=num_epochs) #(dataset, this epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scatter plots of reconstructed values v/s target\n",
    "\n",
    "When there is a perfect match between the prediction and target, the scatter plot would be a line along y=x (45 degrees in the first quadrant). We observe similar behaviour in the plot below indicating that the model has is able to predict the target decently. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title\n",
    "# Let's check out the results  \n",
    "((xx, tt), yy) = next(iter(dataset))\n",
    "ypred = model((xx,tt))\n",
    "\n",
    "chosen_idx = 16 # 0 to 31\n",
    "plt.figure(figsize=[12,6])\n",
    "plt.subplot(1,2,1)\n",
    "plt_obj = plt.pcolor(ypred[chosen_idx,:].numpy().reshape(5,5))\n",
    "plt.colorbar(plt_obj)\n",
    "plt.title('Predicted image')\n",
    "print(ypred[0,:].numpy().reshape(5,5))\n",
    "plt.subplot(1,2,2)\n",
    "plt_obj = plt.pcolor(yy[chosen_idx,:].numpy().reshape(5,5))\n",
    "plt.colorbar(plt_obj)\n",
    "plt.title('Target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiffusionReconstruct(BetaDerivatives):\n",
    "  \n",
    "  def __init__(self, betas):\n",
    "    super().__init__(betas)\n",
    "\n",
    "    self.sqrt_recip_alphas_cumprod = tf.math.sqrt(1. / self.alphas_cumprod)\n",
    "    self.sqrt_recipm1_alphas_cumprod = tf.math.sqrt(1. / self.alphas_cumprod - 1)\n",
    "    \n",
    "    # calculations required for posterior q(x_{t-1} | x_t, x_0)\n",
    "    # Variance choice corresponds to 2nd choice mentioned in the paper\n",
    "    self.posterior_variance = self.betas * (1. - self.alphas_cumprod_prev) / (1. - self.alphas_cumprod)  \n",
    "    \n",
    "    \n",
    "    # below: log calculation clipped because the posterior variance is 0 at the beginning of the diffusion chain\n",
    "    self.posterior_log_variance_clipped = tf.constant(np.log(np.maximum(self.posterior_variance, 1e-20)))\n",
    "    self.posterior_mean_coef1 = self.betas * tf.math.sqrt(self.alphas_cumprod_prev) / (1. - self.alphas_cumprod)\n",
    "    self.posterior_mean_coef2 = (1. - self.alphas_cumprod_prev) * tf.math.sqrt(self.alphas) / (1. - self.alphas_cumprod)\n",
    "  \n",
    "  def predict_start_from_noise(self, x_t, t, noise):\n",
    "    \"\"\"\n",
    "    Reconstruct x_0 using x_t, t and noise. Uses deterministic process\n",
    "    \"\"\"\n",
    "    return (\n",
    "        self._gather(self.sqrt_recip_alphas_cumprod, t) * x_t -\n",
    "        self._gather(self.sqrt_recipm1_alphas_cumprod, t) * noise\n",
    "    )\n",
    "\n",
    "  def q_posterior(self, x_start, x_t, t):\n",
    "    \"\"\"\n",
    "    Compute the mean and variance of the diffusion posterior q(x_{t-1} | x_t, x_0)\n",
    "    \"\"\"\n",
    "    posterior_mean = (\n",
    "        self._gather(self.posterior_mean_coef1, t) * x_start +\n",
    "        self._gather(self.posterior_mean_coef2, t) * x_t\n",
    "    )\n",
    "    posterior_log_variance_clipped = self._gather(self.posterior_log_variance_clipped, t)\n",
    "    return posterior_mean, posterior_log_variance_clipped\n",
    "\n",
    "  def p_sample(self, model, x_t, t):\n",
    "    \"\"\"\n",
    "    Sample from the model. This does 4 things\n",
    "    * Predict the noise from the model using x_t and t\n",
    "    * Create estimate of x_0 using x_t and noise (reconstruction)\n",
    "    * Estimate of model mean and log_variance of x_{t-1} using x_0, x_t and t\n",
    "    * Sample data (for x_{t-1}) using the mean and variance values\n",
    "    \"\"\"\n",
    "    noise_pred = model((x_t, get_timestep_embedding(t, 128))) # Step 1\n",
    "    x_recon = self.predict_start_from_noise(x_t, t=t, noise=noise_pred) # Step 2\n",
    "    model_mean, model_log_variance = self.q_posterior(x_start=x_recon, x_t=x_t, t=t) # Step 3\n",
    "    noise = noise_like(x_t.shape)\n",
    "    nonzero_mask = tf.reshape(tf.cast(tf.greater(t, 0), tf.float32), (x_t.shape[0], 1)) \n",
    "    return model_mean + tf.exp(0.5 * model_log_variance) * noise * nonzero_mask # Step 4\n",
    "\n",
    "  def p_sample_loop_trajectory(self, model, shape):\n",
    "    \"\"\"\n",
    "    Generate the visualization of intermediate steps of the reverse of diffusion\n",
    "    process.\n",
    "    \"\"\"\n",
    "    times = tf.Variable([self.num_timesteps - 1], dtype=tf.int32)\n",
    "    imgs = tf.Variable([noise_like(shape)])\n",
    "    times, imgs = tf.while_loop(\n",
    "      cond=lambda times_, _: tf.greater_equal(times_[-1], 0),\n",
    "      body=lambda times_, imgs_: [\n",
    "        tf.concat([times_, [times_[-1] - 1]], axis=0),\n",
    "        tf.concat([imgs_, [self.p_sample(model=model,\n",
    "                                         x_t=imgs_[-1],\n",
    "                                         t=tf.fill([shape[0]], times_[-1]))]], \n",
    "                  axis=0)\n",
    "      ],\n",
    "      loop_vars=[times, imgs],\n",
    "      shape_invariants=[tf.TensorShape([None, 1]),\n",
    "                        tf.TensorShape([None, *shape])],\n",
    "      back_prop=False\n",
    "    )\n",
    "    return times, imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec_diff = DiffusionReconstruct(betas_quad)\n",
    "pred_ts, pred_data = rec_diff.p_sample_loop_trajectory(model, shape=(1000,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.17 ('tf')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cc84e581e0e462d1381a309e50ab4fd7bcecc9b26d5a2793df3c17ebee9c083d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
